{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import openpyxl\n",
    "import time\n",
    "\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get date from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the API's uber-helful X-Status-Reason in the unlikely event \n",
    "# that we make a bad request\n",
    "def handle_request_error(response, query_url, api_key):\n",
    "    if response.status_code == 400:\n",
    "        x_header_value = response.headers.get(\"X-Status-Reason\")\n",
    "        if x_header_value:\n",
    "            print(f\"Error: {x_header_value}\")\n",
    "        else:\n",
    "            print(\"400 Bad Request: No X-Status-Reason found\")\n",
    "        exit(1)\n",
    "    elif response.status_code == 429:\n",
    "        print(\"Throttled response from the api, retrying in {} seconds\".format(response.headers[\"Retry-After\"]))\n",
    "        time.sleep(int(response.headers[\"Retry-After\"]))  # Number of seconds to wait before sending next request\n",
    "        response = requests.get(query_url, headers={\"X-Api-Key\": api_key})\n",
    "        return response\n",
    "    else:\n",
    "        response.raise_for_status()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keys/patsviews_key', 'r') as file:\n",
    "    api_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://search.patentsview.org' # this is the same for all endpoints\n",
    "endpoint = 'api/v1/patent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"FW\", \"LC\", \"LIB\",  \"SC\", \"SPV\", \"Wind\"]  # technology file names\n",
    "# names = [\"Wind\"]  # technology file names\"Wind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "for n in names:\n",
    "    df = pd.read_excel(f'unlabeled_data/DB_{n}_USPTO.xlsx')\n",
    "    all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_get_request(endpoint, param_dict):\n",
    "    param_string = \"&\".join([f\"{param_name}={json.dumps(param_val)}\" for param_name, param_val in param_dict.items()])\n",
    "    # note the json.dumps to ensure strings are surrounded by double quotes instead of single\n",
    "    # the api will not accept single quotes\n",
    "    query_url = f\"{base_url}/{endpoint.strip('/')}/?{param_string}\"\n",
    "    response = requests.get(query_url, headers={\"X-Api-Key\": api_key})\n",
    "    response = handle_request_error(response, query_url, api_key)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting date extraction for FW\n",
      "Throttled response from the api, retrying in 15 seconds\n",
      "Throttled response from the api, retrying in 1 seconds\n",
      "Throttled response from the api, retrying in 15 seconds\n",
      "saved file for date extraction for FW\n",
      "starting date extraction for LC\n",
      "Throttled response from the api, retrying in 16 seconds\n",
      "Throttled response from the api, retrying in 17 seconds\n",
      "saved file for date extraction for LC\n",
      "starting date extraction for LIB\n",
      "Throttled response from the api, retrying in 15 seconds\n",
      "Throttled response from the api, retrying in 16 seconds\n",
      "Throttled response from the api, retrying in 17 seconds\n",
      "Throttled response from the api, retrying in 15 seconds\n",
      "Throttled response from the api, retrying in 18 seconds\n",
      "Throttled response from the api, retrying in 16 seconds\n",
      "Throttled response from the api, retrying in 14 seconds\n",
      "Throttled response from the api, retrying in 14 seconds\n",
      "Throttled response from the api, retrying in 1 seconds\n",
      "Throttled response from the api, retrying in 14 seconds\n",
      "Throttled response from the api, retrying in 1 seconds\n",
      "Throttled response from the api, retrying in 15 seconds\n",
      "saved file for date extraction for LIB\n",
      "starting date extraction for SC\n",
      "Throttled response from the api, retrying in 17 seconds\n",
      "Throttled response from the api, retrying in 16 seconds\n",
      "Throttled response from the api, retrying in 17 seconds\n",
      "Throttled response from the api, retrying in 18 seconds\n",
      "Throttled response from the api, retrying in 17 seconds\n",
      "Throttled response from the api, retrying in 16 seconds\n",
      "Throttled response from the api, retrying in 18 seconds\n",
      "Throttled response from the api, retrying in 17 seconds\n",
      "saved file for date extraction for SC\n",
      "starting date extraction for SPV\n",
      "Throttled response from the api, retrying in 16 seconds\n",
      "saved file for date extraction for SPV\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for df in all_dfs:\n",
    "    print('starting date extraction for', names[i])\n",
    "    \n",
    "    df.dropna(subset=['Publn_Nr'], inplace=True)  # drop non defined publication numbers\n",
    "    \n",
    "    pub_numbers = df['Publn_Nr'].unique().astype(list)\n",
    "    df_u = df.drop_duplicates(subset=['Publn_Nr'])  # unique claims (sort by patent)\n",
    "    pub_numbers = pub_numbers.astype(int).astype(str)\n",
    "\n",
    "    split_pubs = [pub_numbers[i:i+100] for i in range(0, len(pub_numbers), 100)]  # split into chunks of 100 cause of API limits\n",
    "\n",
    "    # run the API request\n",
    "    list_of_dfs = []\n",
    "    for pub_nrs in split_pubs:\n",
    "        param_dict = {\n",
    "            \"f\" : [\"patent_id\", \"patent_date\"],  # get patent id and date\n",
    "            \"o\" : {\"size\":200}, \n",
    "            \"q\" : {\"patent_id\": list(pub_nrs)},  # insert publication id\n",
    "            # \"s\" : [{\"patent_date\":\"asc\"}], \n",
    "        }\n",
    "        response = make_get_request(endpoint, param_dict)\n",
    "        response_unpacked = pd.DataFrame(response.json()[response_key(endpoint)])\n",
    "        list_of_dfs.append(response_unpacked)\n",
    "\n",
    "    df_dates = pd.concat(list_of_dfs)\n",
    "    df_dates[\"patent_date\"] = pd.to_datetime(df_dates[\"patent_date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "    # save to file\n",
    "    n = names[i]\n",
    "    df_dates.to_csv(f'unlabeled_data/DB_{n}_USPTO_dates.csv', index=False)\n",
    "    print('saved file for date extraction for', n)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge date into unlabled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first sort data sets according to id number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"FW\", \"LC\", \"LIB\",  \"SC\", \"SPV\", \"Wind\"]  # technology file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in names:\n",
    "    # load df\n",
    "    df_dates = pd.read_csv(f'unlabeled_data/DB_{n}_USPTO_dates.csv')\n",
    "    df = pd.read_excel(f'unlabeled_data/DB_{n}_USPTO.xlsx')\n",
    "\n",
    "    # moodify and sort\n",
    "    df.dropna(subset=['Publn_Nr'], inplace=True)  # drop non defined publication numbers\n",
    "    df['Publn_Nr'] = df['Publn_Nr'].astype(int)   # convert to int\n",
    "    df.sort_values(by='Publn_Nr', inplace=True)   # sort by patent number\n",
    "    df['Publn_Date'] = None  # add empty column for dates\n",
    "    df_dates['patent_id'] = df_dates['patent_id'].astype(int)  # convert to int and then to string\n",
    "    df_dates.sort_values(by='patent_id', inplace=True)  # sort by patent number\n",
    "\n",
    "    # iterate over the rows and insert the dates\n",
    "    for idx, row in df_dates.iterrows():\n",
    "        df.loc[df['Publn_Nr'] == row['patent_id'], 'Publn_Date'] = row['patent_date']\n",
    "    \n",
    "    # sort by date\n",
    "    df.sort_values(by='Publn_Date', inplace=True)\n",
    "\n",
    "    # save to file\n",
    "    df.to_excel(f'sorted_data/DB_{n}_USPTO_sorted.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
